x#+TITLE:     Building an application for the Analytics of Virtual Labs
#+AUTHOR:    M.S.Soumya
#+DATE:      2015-06-09 Tue
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

* Introduction
  Web analytics is used to measure the web traffic which helps in
  knowing the site visitor count and several others values like the
  number of page views, resources downloaded etc. This model describes
  a simple web application which displays the analytics of
  virtual-labs.
  
* Requirements
** Requirement #1
   1. The "visitor page views" count of the labs deployed on Amazon
      web services(AWS) and IIIT infrastructure from both AWS and
      local deployment must be displayed on vlab.co.in
   2. The "visitor page views" count must be updated every one hour.
** Requirement #2
   Display the total =usage= of virtual-labs. 
    - Definition of =usage= :: 1 usage (instance) is loading content
         and simulation of one experiment.

** Requirement #3
   Display the usage at an experiment level.
   
** Requirement #4
   Display the lab-wise analytics.

* Design
** Design description
   This model builds a web service that provides endpoints to get
   different sets of information from the statistics generated by
   awstats and apache logs. A four tier architecture is built with the
   following components :
   - data : the specific data based on the requirement.
   - service : data processing and service to provide data
   - view : presentation of data.
   The tiers in the architecture are :
   + Collation of data
   + Extraction of data
   + Service to provide data
   + Presentation
   
   The design of the architecture is depicted in the diagram below.
   
   #+CAPTION:  Design diagram
   #+LABEL:  Design diagram
   [[./diagrams/analytics.png]]

*** Collation of data
    Initially all the labs were hosted from VLEAD infrastructure.  For
    scalability the hosting of labs shifted to Amazon Web Services
    (AWS). The statistics for labs deployed on both these platforms
    were collected. Later the deploy container was decommissioned and
    certain labs were moved to individual containers. More details
    about the deploy decommissioning project can be found [[https://bitbucket.org/vlead/vlead/src/8849aa7d1a44b25dee4ac1b88f4c5f4327e8f2a1/projects/cleaning-deploy/?at%3Dmaster][here]].

    Currently the labs are hosted in 3 different platforms namely:
    + AWS ( Currently 63 labs)
    + Containers at VLEAD (Around 20 labs) 
    + Rest of the labs directed by vlab.co.in (either hosted by Amrita
      or individual institutes)
    
    The statistics of the labs hosted on AWS and the containers at
    VLEAD are obtained because VLEAD has configured awstats on both
    these platforms.  The statistics of the rest of the labs which are
    directed by vlab.co.in (not on AWS or on the VLEAD containers) are
    not obtained.

    #+CAPTION:  data collation design diagram
    #+LABEL:  data collation design diagram
    [[./diagrams/file-flow-diagram.png]] 

    The diagram above shows the two platforms and how the data is
    collated from them into a single VM. The required files from the
    reverse proxy on both the platforms is transferred to the VM
    running the analytics service every one hour.

*** Extraction of data
    During lab usage all the traffic ends up at the reverse
    proxy. Awstats is configured on the reverse proxy. All the lab
    usage data is captured by the reverse proxy in the form of logs.

**** How data is generated
     Awstats is a log analyzer which parses the logs and generates the
     analytics.  Since the logs are stored in the reverse proxy,
     awstats is installed on the same machine.  Awstats generates text
     files which contain the statistics in text format.  For each lab,
     a new gets generated each month.  The daily statistics are
     updated in the monthly text file.  The required data from these
     text files needs to be extracted. Also the access log of the web
     server (apache) are required to extract certain data.
**** How the data is processed
     To extract the required information scripts are written.  For
     [[Requirement%20#1][requirement #1]] the script does the following:
      + parse the awstats data files
      + extract the =number of pages viewed= from each data file
      + Sum up all such figures from all the files in AWS
      + Sum up all such figures from the specific files in VLEAD
      + Total the number of pages viewed from AWS and VLEAD.
     For [[Requirement%20#2][requirement #2]] the script does the following:
      + Convert the apache logs into csv format and save them as csv
        files
      + From the csv file, the tuples with the experiment urls need to
        be extracted.
      + From these tuples, the a combination of the IP and time is to
        be looked at. For those tuples with the same IP, if the time
        stamps differ in a range of (max)15 mins, and if any one of
        the tuple consists of the simulation pattern, then we count
        this as one usage.
      + The usage is taken from several such log files and the total
        is calculated across all the labs and all the experiments.

*** Service to provide data
    A service exposes different endpoints. Currently the endpoint that
    is exposed provides total number of pages. To provide this data
    the service invokes the logic built into the data extraction.
*** Presentation
    To display the endpoints of the service scripts are required which
    fetches the data from the service.

** Design decisions
*** Collation of statistics
    The statistics are present in two different locations, Amazon Web
    Services (AWS) and on the reverse proxy at VLEAD. To get numbers
    the data is required from both these locations. It was decided
    that on an AWS VM the data from both locations would be
    collated. The awstats data from both platforms will be transferred
    on an hourly basis to this VM.
    
*** Where the service will run
    There were concerns on where the service should run. Initially it
    was being assumed that the service would run on the reverse proxy
    server itself. This was so because the stats file were located on
    this machine. By doing so it would reduce the overhead of
    transferring the stats files into another location and also the
    data that the service would use might be stale.  Setting up the
    service on the reverse proxy had its own security threats.  So, it
    was decided that the service will be setup on a VM which belongs
    to the AWS cluster.
*** Setup of the service
    As mentioned in the [[Where%20the%20service%20will%20run][Where the service will run]], a VM on AWS will
    be created to run the service. The VM will be a part of the AWS
    cluster. The concern in setting up this VM is that the other VM’s
    in the cluster are setup by ansible scripts. To setup this service
    ansible scripts need to be written. Manual entry of the FQDN and
    IP into some servers like reverse proxy and DNS may work
    temporarily but when the scripts are run, this information will be
    erased. To avoid this, this VM should be modelled as a node and
    added to the systems-model with the necessary modifications
    involved in setting up this VM.

*** Extraction of usage 
    There were concerns on how the usage patterns can be
    extracted. The 
* Implementation
** Data collation
   
   We have two different sources, a reverse proxy on AWS
   cluster(Source 1) and a reverse proxy on the base machines in IIIT-H
   (Source 2). These sources are geographically located in different
   continents and are on different networks.

   We need a way of transferring the data(statistics) to the analytics
   server where it will be processed. We have used =rsync= to transfer,
   and have setup periodic transfer jobs using =cronie= which executes
   every hour.

   As this task involves collecting data from different sources, like a
   cluster which is managed with configuration management tool =Ansible=
   and another one a manually configured server, data collation is also a
   mix of automation and manual steps.

   We now present the configuration procedure of both.

*** Re-configuring AWS cluster, requirements of analytics node
 - Analytics server configured to accept =rsync= over =TCP=
 - Reverse Proxy(Source 1) configured to push the generated statistics
   to =Analytics= server at regular intervals
 - Router cofigured to allow incoming =rsync= connections from Source
   2 to Analytics server
 
    As AWS cluster is configured by =Ansible=, we have scripts to achieve
    the above via =Ansible=.

*** On BASE
    Here we manually setup:
 - Reverse Proxy(Source 2) configued to push generated statistics to
   =Analytics= server every hour
   
*** TODO
 -  Find a way to rsync after/before awstats update script, so the
    chances of file corruption are minimized.

** Data extraction
*** Page-hits extraction
    The purpose of the script is described in [[How%20the%20data%20is%20processed][How the data is
    processed]] for [[Requirement%20#1][requirement #1]]. The python script is as described
    below.  The necessary packages are imported and some global
    variables are declared:
#+BEGIN_SRC python :tangle src/analytics/extract_data.py
import sys
import re
import json
import os
import fnmatch
__dict_tot_pages__ = {}
all = [__dict_tot_pages__ ]
#+END_SRC
#+BEGIN_SRC python :tangle src/analytics/extract_data.py
def get_file_names(year,location):
    list_of_files = []
    for file in os.listdir(location):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(location + file)
    return list_of_files
#+END_SRC
    There are several awstats files. Some are ".bkp" files and some
    are ".txt" files. We only require the ".txt" files. The following
    function gets all the required awstats text files from the
    specified directory.  From the data collected from AWS, the
    awstats are lab specific. In AWS, awstats is not configured to
    collect the analytics for all the labs together. So, all the
    awstats data files need to be parsed to obtain the page views. The
    function below looks through all the files and adds the page
    views. It returns the total page views of labs deployed on AWS
    infrastructure.
#+BEGIN_SRC python :tangle src/analytics/extract_data.py
def extract_aws_data(year,location):
    files = get_file_names(year,location);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        f = open(i)
        for line in f:
            if re.match(r'^BEGIN_DAY',line):
                n = int((line.strip('BEGIN_DAY ')).rstrip())
                x = 0
                p = 0
                for page_line in f:
                    if (x >= n):
                        break
                    p += int(page_line.split()[1])
                    x += 1
                f.close()
                dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z
#+END_SRC
    Awstats on the reverse proxy is configured to capture the
    analytics of all the labs at a single place. These files are named
    as =awstats'month-year'virtual-labs.ac.in.txt=. The page views
    from these files are only required. The function below looks
    through only the specified files to obtain the page views. It
    returns the total page views of labs deployed on IIIT
    infrastructure.
#+BEGIN_SRC python :tangle src/analytics/extract_data.py
def extract_iiit_data(year, location):
    files = get_file_names(year,location);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if re.search(r'awstats\d+.virtual-labs.ac.in.txt$',i):
            print i
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z
#+END_SRC
    The total page views from both the platforms is obtained from the
    function below. The statistics were collected from the year
    2013-2015 hence the years are passed as parameters to the above
    functions.
#+BEGIN_SRC python :tangle src/analytics/extract_data.py
def grand_total():
    global __dict_tot_pages__
    year = [ '2013','2014', '2015' ]
    aws_location = '/root/aws/'
    base_location = '/root/base/'
    tot_pages=0
    for i in year:
        tot_pages += extract_iiit_data(i,base_location) + extract_aws_data(i,aws_location)
    __dict_tot_pages__["total-pages"]=str(tot_pages)
    return __dict_tot_pages__["total-pages"]
#+END_SRC

*** Total usage extraction 
    The purpose of this script is described in [[How%20the%20data%20is%20processed][How the data is
    processed]] for [[Requirement%20#2][requirement #2]]. The python script is as below.
#+BEGIN_SRC 
import shlex
import os
import csv
import fnmatch
import re

def separator():
    files_list = []
    location = '/home/ms/systems/analytics/usage-service/logs/'
    #files_list.append(os.system('ls -d -1 $PWD/logs/*.*'))
    #files_list.pop(len(files_list) -1)
    for file in os.listdir(location):
        if fnmatch.fnmatch(file, '*'):
            files_list.append(location + file)
    print files_list
    csv_file = open('cse01.csv', 'wb')
    open_csv = csv.writer(csv_file)
    inputFile = open(files_list[1])
    fileContent = inputFile.readlines()
#    print fileContent
    #csv_output = list()
    for line in fileContent:
        output = ','.join(shlex.split(line)) + '\n'
#        print ','.join(shlex.split(line))
        csv_file.write(output)
#    print csv_output
    csv_file.close()

def usage(x):
    exp_match_list = []
    csv_file = open('test.csv', 'r')
    rows = csv_file.readlines()
    for row in rows:
        if re.search(x,row):
#            print row
            exp_match_list.append(row)
    print exp_match_list
#    return

#    print exp_match_list

exp_url=['http://deploy.virtual-labs.ac.in/labs/cse04/exp1/index.php']
filter(usage, exp_url)
#+END_SRC
    
** Building the Service
   + For building the service, python-flask is used.
   Flask is a micro web application framework written in Python.
   more details about flask can be found [[http://flask.pocoo.org/docs/0.10/][here]].

   + This service, in its present state, will parse the data, extract
   aws-stats, calculate and return the total number of page visits.
   It exposes one endpoint =/numberofhits=.  An "endpoint" is an 
   identifier that is used in determining what logical unit of code
   should handle the request. 

#+BEGIN_SRC python :tangle src/analytics/app.py
from flask import Flask, render_template, request, jsonify, make_response
# import the flask extension
from flask.ext.cache import Cache
import json
#import os
#import config
#from data import extract_data
#from data.extract_data import *
from extract_data import *
#+END_SRC

Create an instance of the class(Flask). The argument is the name of
the application’s module or package.  This is needed so that Flask
knows where to look for templates, static files, and so on.

#+BEGIN_SRC python :tangle src/analytics/app.py
app = Flask(__name__)
#+END_SRC

  Define the cache config keys.

#+BEGIN_SRC python :tangle src/analytics/app.py
app.config['CACHE_TYPE'] = 'simple'
#+END_SRC

  Register the cache instance and bind it to app. 

#+BEGIN_SRC python :tangle src/analytics/app.py
app.cache = Cache(app)
#+END_SRC

  Register the cache instance and bind it to app. 

Use the =route()= decorator to tell Flask what URL should trigger the
function.

#+BEGIN_SRC python :tangle src/analytics/app.py
@app.route('/numberofhits')
#+END_SRC

 Cache this view for 1 hour. This is needed because the service must
 display the updated count every one hour.

#+BEGIN_SRC python :tangle src/analytics/app.py
@app.cache.cached(timeout=360)  
#+END_SRC

 The function is given a name which is also used to generate URLs
 for that particular function, and returns the value we want to
 display in the browser.
 
#+BEGIN_SRC python :tangle src/analytics/app.py
def numberofhits():
#+END_SRC

 Call the function =grand_total()= which is defined in
 =extract_data.py= to get the total number of hits.
  
#+BEGIN_SRC python :tangle src/analytics/app.py
    numberofhits  = grand_total()
#+END_SRC

   The =make_response()= function can be called instead of using a return
   and will get a response object which can be used to attach headers.
   =Access-Control-Allow-Origin= is a CORS (Cross-Origin Resource
   Sharing) header.  When Site A tries to fetch content from Site B,
   Site B can send an =Access-Control-Allow-Origin= response header to
   tell the browser that the content of this page is accessible to
   certain origins.  By default, Site B's pages are not accessible to
   any other origin, using the =Access-Control-Allow-Origin= header
   opens a door for cross-origin access by specific requesting
   origins.  The server can give permission to include cookies by
   setting the =Access-Control-Allow-Credentials= header.

#+BEGIN_SRC python :tangle src/analytics/app.py
    response = make_response(numberofhits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC

   Finally, use the =run()= function to run the local server with
   our application. The if *__name__ == '__main__':* makes sure the
   server only runs if the script is executed directly from the Python
   interpreter and not used as an imported module.  

#+BEGIN_SRC python :tangle src/analytics/app.py
if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC
*** Provisioning the Service
***** Machine Configuration
   + Operating System: centos-6.6
   + Architecture: x86_64
   + Memory: 256 MB
   + Disk space: 10 GB
   + Interface: venet0
***** Steps to manually create a centos container 
#+BEGIN_SRC 
vzctl create 16133 --ostemplate centos-6-x86_64-point6 --ipadd 10.4.15.133 
--diskspace 10G:15.0G --hostname stats-demo.vlabs.ac.in
vzctl start 16133
vzctl set 16133 --nameserver inherit --ram 256M --swap 512M --onboot yes --save
#+END_SRC
***** Export proxy Settings
#+BEGIN_SRC 
export http_proxy="proxy.iiit.ac.in:8080"
export https_proxy="proxy.iiit.ac.in:8080"
#+END_SRC    
***** Update the System
    In order to have a stable deployment server, it is crucial to keep things up-to-date and well maintained.
    To ensure that we have the latest available versions of default applications, we need to update our system.
    Run the following command to update your system
#+BEGIN_SRC 
sudo yum -y update
#+END_SRC
***** Install virtualenv
    Run the following command to download and install virtualenv using pip.
    =virtualenv= is a tool to create isolated Python environments.
#+BEGIN_SRC 
sudo pip install virtualenv
#+END_SRC
***** Install epel For RHEL 6.x and CentOS 6.x (x86_64)
#+BEGIN_SRC 
rpm -ivh http://dl.fedoraproject.org/pub
/epel/6/x86_64/epel-release-6-8.noarch.rpm
#+END_SRC
***** Install pip with yum command
#+BEGIN_SRC 
yum install -y python-pip
#+END_SRC
***** Install Flask
   Enter the following command to get Flask activated in your =virtualenv=
#+BEGIN_SRC 
 pip install Flask
#+END_SRC
***** Install Flask-Cache
#+BEGIN_SRC 
 pip install Flask-Cache
#+END_SRC
***** Install wsgi on CentOS using yum
   WSGI(Web Server Gateway Interface) is an interface between a web server 
   and the application itself. It exists to ensure a standardized way 
   between various servers and applications (frameworks) to work with each 
   other, allowing interchangeability when necessary (e.g. switching from 
   development to production environment).
#+BEGIN_SRC 
yum install mod_wsgi
#+END_SRC

***** Create a =.wsgi= file
    To run your application you need a =analytics.wsgi= file. 
    This file contains the code =mod_wsgi= is executing on startup to get the application object. 
    The object called application in that file is then used as application.
#+BEGIN_SRC  python :tangle src/analytics/analytics.wsgi
import sys
sys.path.insert (0,'/var/www/html/analytics/')

import logging, sys
logging.basicConfig(stream=sys.stderr)

from app import app as application
 #+END_SRC
***** Configure the Apache and deploy the service
      Configure Apache to load =mod_wsgi= module and your project in VirtualHost
      Insert the following lines in =/etc/httpd/conf/httpd.conf=
#+BEGIN_SRC 
WSGIScriptAlias / /var/www/html/analytics/analytics.wsgi
WSGIScriptReloading On
<Directory /var/www/html/analytics>
     Order deny,allow
     Allow from all
 </Directory>

#+END_SRC
***** Restart Apache
#+BEGIN_SRC 
service httpd restart
#+END_SRC
***** Test the service with end point.
#+BEGIN_SRC 
http://10.4.15.133/numberofhits
#+END_SRC

** Presenting the data
   For presenting the data, AJAX (Asynchronous JavaScript and
   XML)and HTML(HyperText Markup Language) is used. 
   AJAX allows web pages to be updated asynchronously by
   exchanging small amounts of data with the server. This means that
   it is possible to update parts of a web page, without reloading the
   whole page. The code snippet below will call the endpoint
   =/numberofhits=, where it will return the total number of pages
   visited as a response and displays on browser.

#+BEGIN_SRC html :tangle src/analytics/test-stats/index.html 
<html>
   <head>
      <title>Analytics</title>
      <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
      <script type="text/javascript" language="javascript">
         $(document).ready(function() {
               $.getJSON('http://10.4.15.133/numberofhits', function(data) {
                  $("#analytics").html("<p> Total number of Page Hits Since 2013: " + data.numberofhits + "</p>");
               });
         });
      </script>
   </head>
   <body>
      <div id="analytics">
         Analytics
      </div>
   </body>
</html>
#+END_SRC

   Note: If you want to display the total number of page visits on vlab.co.in, 
   then add the above code snippet in =index.html= file. 
* Test Cases
  A test case is a document, which has a set of test data,
  preconditions, expected results and post-conditions, developed for a
  particular test scenario in order to verify compliance against a
  specific requirement.
** Objective:
   The objective of this test cases is to test whether the application 
   is running or not, by sending HTTP GET request and also to test 
   the return value of the response. 
** Test case ID: TC01
** Test case name: Test Number of Page Visits
** Test case description:
   In this test case, end point =/numberofhits= is tested, where it 
   will return the total number of pages visited.
** Test data/Input data:
   The Input data is required to test the test case. 
   Input data is to verify that a given set of 
   input to a given function/program produces some expected result. 
   Input can be valid data or invalid data.
   Here, the input data will return the user-defined value when
    =grand_total()= function is called. 
#+BEGIN_SRC python:tangle tests/sample_data.py
#!/usr/bin/python
def grand_total():
  dict = {'numberofhits':'1234'};
  return dict['numberofhits']
#+END_SRC
** Step description/action
*** Step 1:
    Import the required packages used for testing.
#+BEGIN_SRC python :tangle tests/test_app.py
import unittest
from flask.ext.testing import TestCase
import sample_data
from app import app
#+END_SRC
*** Step 2:
 A test case is created by sub-classing =unittest.TestCase=.
#+BEGIN_SRC python :tangle tests/test_app.py 
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
*** Step 3:
    The tests are defined with methods whose names start with the
    letters =test=.  This naming convention informs the test runner
    about which methods represent tests.  To start testing the
    functionality of the application, add a new test method to our
    class, like =test_numberofhits()=.Similarly, other test methods
    can be added. 
#+BEGIN_SRC python :tangle tests/test_app.py 
    def test_numberofhits(self):
        json_string = sample_data.grand_total()
        print json_string
#+END_SRC
*** Step 4:
   Each test is a call to =assertEqual()= function which will test
   that first and second arguments are equal.  If the values do not
   compare equal, then test will fail.
   - The status code of the endpoint =/numberofhits= is equated to =200= 
   which means =OK= (the request has succeeded). The endpoint returned 
   with the response.
   - The data which is returned in the form of response is compared against
     the Input data.
#+BEGIN_SRC python :tangle tests/test_app.py 
        tester = app.test_client(self)
        response = tester.get('http://localhost:5000/numberofhits')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(json_string,  response.data)

#+END_SRC            
*** Step 5:
    The final block shows a simple way to run the
    tests. =unittest.main()= provides a command-line interface to the
    test script.  When run from the command line, the script
    produces an output.
#+BEGIN_SRC python :tangle tests/test_app.py 
if __name__ == '__main__':
    unittest.main()
#+END_SRC

** Expected Result
    - Run =python test_app.py= in terminal.
    - Check the output, if it was like =Ran 1 test in 0.034s= and =OK=.
    - If the output is found then test is passed.
 #+BEGIN_SRC 
$ python test_app.py
.
----------------------------------------------------------------------
Ran 1 test in 0.034s

OK
#+END_SRC

* Releases
** Release v1.0.0
   This release realizes [[Requirement #1]].
   The release date is [2015-07-08 Wed]
*** Work Plan
**** TODO Prepare the document
**** DONE Discuss on how to meet the requirement
          There was a discussion on [2015-06-23 Tue] with the
          participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
          the other VLEAD members. The task division and the process to
          bring up the service was also discussed which is mentioned in
          the [[Process%20for%20implementation][Process for implementation]] section.
**** DONE Figure out the various location of the labs
          There are several locations the labs are situated namely AWS,
          deploy, separate containers. How to display the combined
          output of all the labs is yet to be decided.  (This has been
          done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
          scenario of statistics]])

**** DONE Build the scripts
**** DONE Test the scripts.
**** TODO Updation of vlab.co.in

** Release v1.0.2
   This release realizes [[Requirement%20#2][Requirement #2]].
   The release date is:
*** Work Plan
**** TODO Update model with usage design [[https://github.com/vlead/analytics/issues/2][#2]]
**** TODO Write script  to convert logs into csv format [[https://github.com/vlead/analytics/issues/9][#9]]
**** TODO Write a python script to extract usage form the CSV formatted logs. [[https://github.com/vlead/analytics/issues/8][#8]]
**** TODO Update model with usage extraction script implementation [[https://github.com/vlead/analytics/issues/12][#12]]
**** TODO Add a new endpoint to the service [[https://github.com/vlead/analytics/issues/6][#6]]
**** TODO A display page to show the usage [[https://github.com/vlead/analytics/issues/7][#7]]
**** TODO Update model with html, ajax and js implementation [[https://github.com/vlead/analytics/issues/13][#13]]
**** TODO Update model with service implementation [[https://github.com/vlead/analytics/issues/3][#3]]
**** TODO Write test cases [[https://github.com/vlead/analytics/issues/5][#5]]
**** TODO Update model with test-cases [[https://github.com/vlead/analytics/issues/4][#4]]
**** TODO Add usage definitions to pop-up page [[https://github.com/vlead/analytics/issues/14][#14]]

* COMMENT  introduction
  The analytics of Virtual-labs are currently generated by awstats,
  which is an on-site web analytics technology. Awstats provides its
  own dashboard for viewing statistics.  It provides a layman with too
  much information to comprehend.  A simple dashboard which displays
  the total views and hits of the site is required. This would help
  keep us informed on the usage of our labs.

* COMMENT Design
   The aim of this model is to build a web service which
   would display certain specific information from the statistics
   generated by awstats. As the diagram suggests, a script will be
   embedded on vlab.co.in, which contains the URL of the service. This
   would invoke the service running on our server which would display
   the total number of pages. The service in turn requires the parsed
   files generated by awstats. These files need to be periodically
   updated to obtain the latest data from the logs. From different
   locations, these files are transferred into the service machine.
  
* COMMENT Implementation
** Procedure
   After a discussion (which also involved exchange of mails) the
   process of implementation was decided as follows:
   1. Pull all the data from deploy and AWS
      - Collate data from deploy and AWS
      - This data that will be used by the service using helper functions.
   2. Serving the data. 
      - Define end points based on the requirements
      - Build a service that implements these end points.
   3. Rendering of the data on vlab.co.in
      - Insert the JavaScript snippet in vlab.co.in
   4. Backup of all the statistics
       
  A service needs to run which would invoke the script written above
  periodically to obtain the latest data. Also the service is required
  to display the data on a html page.
  
  *Note*: Currently the HTML page is on vlab.co.in home page. In
  future a button / hyperlink could also be incorporated which would
  display a page that shows slightly more details.
    
